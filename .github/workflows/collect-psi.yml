name: Coleta PSI (PageSpeed Insights)

on:
  schedule:
    - cron: '0 3 * * *'  # Todo dia as 03:00 UTC
  workflow_dispatch:
    inputs:
      max_concurrent:
        description: 'Requisicoes simultaneas'
        required: false
        default: '10'
        type: string
      requests_per_second:
        description: 'Requisicoes por segundo (max 4.0)'
        required: false
        default: '3.5'
        type: string
      skip_recent_hours:
        description: 'Pular sites auditados nas ultimas N horas (0=todos)'
        required: false
        default: '24'
        type: string

env:
  PAGESPEED_API_KEY: ${{ secrets.PSI_KEY }}
  IA_ACCESS_KEY: ${{ secrets.IA_ACCESS_KEY }}
  IA_SECRET_KEY: ${{ secrets.IA_SECRET_KEY }}
  DATABASE_PATH: data/sites_prefeituras.duckdb
  OUTPUT_DIR: data/output

jobs:
  validate:
    name: Validar Configuracao
    runs-on: ubuntu-latest
    steps:
      - name: Check PSI API Key
        run: |
          if [ -z "${{ secrets.PSI_KEY }}" ]; then
            echo "::error::PSI_KEY secret nao configurado!"
            echo "Configure o secret PSI_KEY com sua Google PageSpeed Insights API key"
            exit 1
          fi
          echo "PSI_KEY configurado corretamente"

      - name: Check Internet Archive Keys (opcional)
        run: |
          if [ -z "${{ secrets.IA_ACCESS_KEY }}" ] || [ -z "${{ secrets.IA_SECRET_KEY }}" ]; then
            echo "::warning::IA_ACCESS_KEY ou IA_SECRET_KEY nao configurados"
            echo "Upload para Internet Archive sera ignorado"
          else
            echo "Credenciais do Internet Archive configuradas"
          fi

  collect:
    name: Coletar Dados PSI
    needs: validate
    runs-on: ubuntu-latest
    # Timeout de 120 minutos e suficiente porque:
    # - Coleta incremental (skip-recent=24h) processa apenas ~100-200 sites
    # - Coleta completa: 5.570 sites ร 2 estrategias รท 3.5 req/s = ~53 min
    # - 120 min fornece margem para retries e overhead
    timeout-minutes: 120

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Necessario para commits

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"

      - name: Install dependencies
        run: uv sync

      - name: Create output directories
        run: mkdir -p data/output data/psi_error_reports

      - name: Restore previous database (if exists)
        uses: actions/cache@v4
        with:
          path: data/sites_prefeituras.duckdb
          key: psi-duckdb-${{ github.run_id }}
          restore-keys: |
            psi-duckdb-

      - name: Run PSI Collection
        id: collect
        continue-on-error: true
        run: |
          # Configurar parametros
          MAX_CONCURRENT="${{ github.event.inputs.max_concurrent || '10' }}"
          RPS="${{ github.event.inputs.requests_per_second || '3.5' }}"
          SKIP_RECENT="${{ github.event.inputs.skip_recent_hours || '24' }}"

          echo "Iniciando coleta PSI (otimizada)..."
          echo "  Max concurrent: ${MAX_CONCURRENT}"
          echo "  Requests/second: ${RPS}"
          echo "  Skip recent: ${SKIP_RECENT}h"

          # Executar coleta via CLI Python com coleta incremental
          uv run sites-prefeituras batch \
            sites_das_prefeituras_brasileiras.csv \
            --output-dir "$OUTPUT_DIR" \
            --max-concurrent "$MAX_CONCURRENT" \
            --requests-per-second "$RPS" \
            --skip-recent "$SKIP_RECENT" \
            --url-column "url" \
            --export-parquet \
            --export-json \
            2>&1 | tee collection.log

          # Verificar se houve erros
          if grep -q "ERROR\|Exception\|Failed" collection.log; then
            echo "has_errors=true" >> $GITHUB_OUTPUT
          else
            echo "has_errors=false" >> $GITHUB_OUTPUT
          fi

      - name: Generate statistics
        if: always()
        run: |
          echo "## Estatisticas da Coleta" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f "$DATABASE_PATH" ]; then
            uv run sites-prefeituras stats --db-path "$DATABASE_PATH" || true

            # Adicionar ao summary
            echo "Database: $DATABASE_PATH" >> $GITHUB_STEP_SUMMARY
            echo "Size: $(du -h $DATABASE_PATH | cut -f1)" >> $GITHUB_STEP_SUMMARY
          else
            echo "Database nao encontrado" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Archive error logs
        if: steps.collect.outputs.has_errors == 'true'
        run: |
          TIMESTAMP=$(date -u +'%Y%m%d_%H%M%S')
          cp collection.log data/psi_error_reports/collection_${TIMESTAMP}.log
          echo "Log arquivado em data/psi_error_reports/collection_${TIMESTAMP}.log"

      - name: Export data for web dashboard (static JSON)
        if: always()
        run: |
          if [ -f "$DATABASE_PATH" ]; then
            echo "Exportando dados para dashboard (JSON estatico)..."

            # Usar CLI para exportar JSONs estaticos
            uv run sites-prefeituras export-dashboard \
              --db-path "$DATABASE_PATH" \
              --output-dir "docs/data" \
              || echo "Dashboard export failed, skipping..."

            # Listar arquivos gerados
            if [ -d "docs/data" ]; then
              echo "Arquivos gerados:"
              ls -la docs/data/
            fi
          fi

      - name: Update and export quarantine list
        if: always()
        run: |
          if [ -f "$DATABASE_PATH" ]; then
            echo "Atualizando lista de quarentena..."

            # Atualizar quarentena (sites com 3+ dias de falha)
            uv run sites-prefeituras quarantine --update --min-days 3 --db-path "$DATABASE_PATH" || true

            # Exportar para JSON (para repositorio e Internet Archive)
            QUARANTINE_DATE=$(date -u +'%Y-%m-%d')
            uv run sites-prefeituras quarantine --export-json "data/quarantine/quarantine-${QUARANTINE_DATE}.json" --db-path "$DATABASE_PATH" || true
            uv run sites-prefeituras quarantine --export-json "data/quarantine/quarantine-latest.json" --db-path "$DATABASE_PATH" || true
            uv run sites-prefeituras quarantine --export-csv "data/quarantine/quarantine-latest.csv" --db-path "$DATABASE_PATH" || true

            # Resumo para o step summary
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "## Quarentena" >> $GITHUB_STEP_SUMMARY
            if [ -f "data/quarantine/quarantine-latest.json" ]; then
              QUARANTINE_COUNT=$(jq '.sites | length' data/quarantine/quarantine-latest.json 2>/dev/null || echo "0")
              echo "Sites em quarentena: ${QUARANTINE_COUNT}" >> $GITHUB_STEP_SUMMARY
            fi
          fi

      - name: Upload to Internet Archive
        if: success() && env.IA_ACCESS_KEY != '' && env.IA_SECRET_KEY != ''
        continue-on-error: true
        env:
          IA_ITEM_IDENTIFIER: psi_brazilian_city_audits
        run: |
          if [ -f "$DATABASE_PATH" ]; then
            echo "Uploading to Internet Archive..."

            # Upload JSONs do dashboard
            if [ -d "docs/data" ]; then
              echo "Uploading dashboard JSON files to Internet Archive..."
              uv run python -m sites_prefeituras.upload_ia dashboard \
                docs/data \
                --item "$IA_ITEM_IDENTIFIER" \
                || echo "Dashboard JSON upload failed"
            fi

            # Upload quarentena diario
            if [ -f "data/quarantine/quarantine-latest.json" ]; then
              echo "Uploading quarantine list to Internet Archive..."
              QUARANTINE_DATE=$(date -u +'%Y-%m-%d')

              uv run python -m sites_prefeituras.upload_ia quarantine \
                "data/quarantine/quarantine-${QUARANTINE_DATE}.json" \
                "data/quarantine/quarantine-latest.json" \
                --item "$IA_ITEM_IDENTIFIER" \
                || echo "Quarantine IA upload failed"
            fi
          fi

      - name: Commit and push results
        if: always()
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          # Add all data files and dashboard JSONs
          git add data/ || true
          git add docs/data/ || true

          # Check if there are changes
          if git diff --staged --quiet; then
            echo "Nenhuma alteracao para commitar"
            exit 0
          fi

          # Create commit message
          TIMESTAMP=$(date -u +'%Y-%m-%d %H:%M UTC')
          if [ "${{ steps.collect.outputs.has_errors }}" == "true" ]; then
            COMMIT_MSG="chore(psi): coleta com erros - $TIMESTAMP"
          else
            COMMIT_MSG="chore(psi): coleta bem-sucedida - $TIMESTAMP"
          fi

          git commit -m "$COMMIT_MSG" -m "Workflow: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"

          # Push with retry
          for i in 1 2 3 4; do
            git push && break
            echo "Push falhou, tentativa $i. Aguardando..."
            sleep $((2 ** i))
          done
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Save database cache
        uses: actions/cache/save@v4
        if: always()
        with:
          path: data/sites_prefeituras.duckdb
          key: psi-duckdb-${{ github.run_id }}

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: psi-results-${{ github.run_id }}
          path: |
            data/output/
            data/quarantine/
            docs/data/
            collection.log
          retention-days: 30
